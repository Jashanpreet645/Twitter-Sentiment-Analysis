# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uKOEhFM6zEuQH8zNZV4qKXrW3q8sSoZ0

# TASK #1: INTRODUCTION TO GUIDED PROJECT

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

data source: https://www.kaggle.com/sid321axn/amazon-alexa-reviews/kernels

# TASK #2: PROJECT WALKTHROUGH, ENHANCED FEATURES, AND LEARNING OUTCOMES

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# PRACTICE OPPORTUNITY #1:

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# TASK #3: IMPORT LIBRARIES AND DATASETS
"""

!pip install jupyterthemes
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from jupyterthemes import jtplot
jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)
# setting the style of the notebook to be monokai theme
# this line of code is important to ensure that we are able to see the x and y axes clearly
# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them.

# Load the data
tweets_df = pd.read_csv('twitter.csv')

tweets_df

tweets_df.info()

tweets_df.describe()

tweets_df['tweet']

# Drop the 'id' column
tweets_df = tweets_df.drop(['id'], axis=1)

"""# TASK #3: PERFORM DATA EXPLORATION"""

sns.heatmap(tweets_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")

tweets_df.hist(bins = 30, figsize = (13,5), color = 'r')

sns.countplot(tweets_df['label'], label = "Count")

# Let's get the length of the messages
tweets_df['length'] = tweets_df['tweet'].apply(len)

tweets_df

tweets_df.describe()

# Let's see the shortest message
tweets_df[tweets_df['length'] == 11]['tweet'].iloc[0]

"""# PRACTICE OPPORTUNITY #2:

![image.png](attachment:image.png)
"""

# Let's view the message with mean length
tweets_df[tweets_df['length'] == 84]['tweet'].iloc[0]

# Plot the histogram of the length column
tweets_df['length'].plot(bins=100, kind='hist')

"""# TASK #4: PLOT THE WORDCLOUD"""

positive = tweets_df[tweets_df['label']==0]
positive

negative = tweets_df[tweets_df['label']==1]
negative

sentences = tweets_df['tweet'].tolist()
len(sentences)

sentences_as_one_string =" ".join(sentences)

sentences_as_one_string

!pip install wordcloud

from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

"""# PRACTICE OPPORTUNITY #3:

![image.png](attachment:image.png)
"""

negative_list = negative['tweet'].tolist()
negative_list
negative_sentences_as_one_string = " ".join(negative_list)
plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))

"""# TASK #5: PERFORM DATA CLEANING - REMOVE PUNCTUATION FROM TEXT"""

import string
string.punctuation

Test = '$I love ##AI & M@achine learning!-_!'
Test_punc_removed = [char for char in Test if char not in string.punctuation]
Test_punc_removed_join = ''.join(Test_punc_removed)
Test_punc_removed_join

Test = 'Good morning beautiful people :)... I am having fun learning Machine learning and AI!!'

Test_punc_removed = [char for char in Test if char not in string.punctuation]
Test_punc_removed

# Join the characters again to form the string.
Test_punc_removed_join = ''.join(Test_punc_removed)
Test_punc_removed_join

"""# TASK 6: PERFORM DATA CLEANING - REMOVE STOPWORDS"""

import nltk # Natural Language tool kit
nltk.download('stopwords')

# You have to download stopwords Package to execute this command
from nltk.corpus import stopwords
stopwords.words('english')

Test_punc_removed_join = 'I enjoy coding, programming and Artificial intelligence'
Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]

Test_punc_removed_join_clean # Only important (no so common) words are left

Test_punc_removed_join

"""# PRACTICE OPPORTUNITY #4:

![image.png](attachment:image.png)
"""

mini_challenge = 'Here is a mini challenge, that will teach you how to remove stopwords and punctuations!'

# Remove punctuations
challege = [ char     for char in mini_challenge  if char not in string.punctuation ]
challenge = ''.join(challege)
challenge

challenge = [  word for word in challenge.split() if word.lower() not in stopwords.words('english')  ]
challenge

"""# TASK 7: PERFORM COUNT VECTORIZATION (TOKENIZATION)

![image.png](attachment:image.png)
"""

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names_out())

print(X.toarray())

"""# PRACTICE OPPORTUNITY #5:

![image.png](attachment:image.png)
"""

mini_challenge = ['Hello World','Hello Hello World','Hello World world world']

# mini_challenge = ['Hello World', 'Hello Hello Hello World world', 'Hello Hello World world world World']

vectorizer_challenge = CountVectorizer()
X_challenge = vectorizer_challenge.fit_transform(mini_challenge)
print(X_challenge.toarray())

"""# TASK #8: CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION"""

# Let's define a pipeline to clean up all the messages
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Let's test the newly added function
tweets_df_clean = tweets_df['tweet'].apply(message_cleaning)

print(tweets_df_clean[5]) # show the cleaned up version

print(tweets_df['tweet'][5]) # show the original version

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning, dtype = np.uint8)
tweets_countvectorizer = vectorizer.fit_transform(tweets_df['tweet'])

print(vectorizer.get_feature_names_out())

print(tweets_countvectorizer.toarray())

tweets_countvectorizer.shape

X = pd.DataFrame(tweets_countvectorizer.toarray())

X

y = tweets_df['label']

"""# TASK #9: UNDERSTAND THE THEORY AND INTUITION BEHIND NAIVE BAYES

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# PRACTICE OPPORTUNITY #6:

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# TASK #10: TRAIN AND EVALUATE A NAIVE BAYES CLASSIFIER MODEL
"""

X.shape

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

"""![image.png](attachment:image.png)"""

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))

"""# FINAL PROJECT

![image.png](attachment:image.png)

![image.png](attachment:image.png)

# FINAL PROJECT SOLUTION TASK #1: IMPORT DATA AND PERFORM EXPLORATORY DATA ANALYSIS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from jupyterthemes import jtplot
jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)
# setting the style of the notebook to be monokai theme
# this line of code is important to ensure that we are able to see the x and y axes clearly
# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them.

# Load the data
reviews_df = pd.read_csv('amazon_reviews.csv')
reviews_df

# View the DataFrame Information
reviews_df.info()

# View DataFrame Statistical Summary
reviews_df.describe()

# Plot the count plot for the ratings
sns.countplot(x = reviews_df['rating'])

# Let's get the length of the verified_reviews column
reviews_df['length'] = reviews_df['verified_reviews'].fillna('').apply(len)

reviews_df

# Plot the histogram for the length
reviews_df['length'].plot(bins=100, kind='hist')

# Apply the describe method to get statistical summary
reviews_df.describe()

# Plot the countplot for feedback
# Positive ~2800
# Negative ~250
sns.countplot(x = reviews_df['feedback'])

"""# FINAL PROJECT SOLUTION TASK #2: PLOT WORDCLOUD"""

# Obtain only the positive reviews
positive = reviews_df[reviews_df['feedback'] == 1]
positive

# Obtain the negative reviews only
negative = reviews_df[reviews_df['feedback'] == 0]
negative

# Convert to list format
sentences = positive['verified_reviews'].tolist()
len(sentences)

# Join all reviews into one large string
sentences_as_one_string =" ".join(sentences)

sentences_as_one_string

from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

sentences = negative['verified_reviews'].fillna('').tolist()
len(sentences)
sentences_as_one_string =" ".join(sentences)
plt.figure(figsize = (20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

"""# FINAL PROJECT SOLUTION TASK #3: PERFORM DATA CLEANING"""

# Let's define a pipeline to clean up all the messages
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Let's test the newly added function
reviews_df_clean = reviews_df['verified_reviews'].fillna('').apply(message_cleaning)

# show the original review
print(reviews_df['verified_reviews'][5])

# show the cleaned up version
print(reviews_df_clean[5])

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning)
reviews_countvectorizer = vectorizer.fit_transform(reviews_df['verified_reviews'].fillna(''))

print(vectorizer.get_feature_names_out())

print(reviews_countvectorizer.toarray())

reviews_countvectorizer.shape

reviews = pd.DataFrame(reviews_countvectorizer.toarray())

X = reviews

y = reviews_df['feedback']
y

"""# FINAL PROJECT SOLUTION TASK #4: TRAIN AND TEST AI/ML MODELS"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm, annot = True)

print(classification_report(y_test, y_pred))

from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm, annot = True)

print(classification_report(y_test, y_pred))

"""# EXCELLENT JOB! YOU SHOULD BE PROUD OF YOUR NEWLY ACQUIRED SKILLS"""

